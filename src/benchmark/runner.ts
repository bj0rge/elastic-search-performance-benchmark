import { Client } from "@elastic/elasticsearch";
import { elasticSdk } from "../sdk";
import {
  createDataGenerator,
  DataGenerator,
} from "../generator/product-generator";
import {
  createStandardIndexConfig,
  createNGramIndexConfig,
  createStemmingIndexConfig,
} from "../config/index-configs";
import {
  createDynamicIndexConfig,
  logIndexConfiguration,
} from "../config/dynamic-index-configs";
import {
  executeIndexingPhase,
  executeUpdatePhase,
  executeSearchPhase,
} from "./phases";
import type { BenchmarkContext } from "./phases/benchmark-context";
import type { BenchmarkConfig, IndexType, Product } from "../types";
import type { IndexingPhaseResult } from "./phases/indexing-phase";
import type { UpdatePhaseResult } from "./phases/update-phase";
import type { SearchPhaseResult } from "./phases/search-phase";
import { buildLogger, DEFAULT_CORPUS_PATH, type Logger } from "../utils";

type BenchmarkRunnerResult = {
  config: BenchmarkConfig;
  indexingData: IndexingPhaseResult;
  updateData: UpdatePhaseResult;
  searchData: SearchPhaseResult;
  totalTestDurationMs: number;
};

type RunnerOptions = {
  corpusPath?: string;
  cleanup?: {
    deleteIndexAfter?: boolean;
    deleteIndexBefore?: boolean;
  };

  verbose?: boolean;

  phases?: {
    indexing?: boolean;
    updates?: boolean;
    search?: boolean;
  };
};

function generateOptionsFromDefaults(
  options: RunnerOptions
): Required<RunnerOptions> {
  return {
    corpusPath: options.corpusPath || DEFAULT_CORPUS_PATH,
    cleanup: {
      deleteIndexBefore: true,
      deleteIndexAfter: false,
      ...options.cleanup,
    },
    verbose: options.verbose ?? true,
    phases: {
      indexing: true,
      updates: true,
      search: true,
      ...options.phases,
    },
  };
}

export const runBenchmark = async (
  config: BenchmarkConfig,
  options: RunnerOptions = {}
): Promise<BenchmarkRunnerResult> => {
  const startTime = Date.now();
  const opts = generateOptionsFromDefaults(options);
  const logger = buildLogger(opts.verbose);

  logger.log(`ðŸš€ Starting benchmark: ${config.indexName}`);
  logger.log(`ðŸ“Š Configuration:`);
  logger.log(`\t- Index type: ${config.indexType}`);
  logger.log(
    `\t- Documents: ${config.numberOfBatches} * ${config.documentsPerBatch} = ${
      config.numberOfBatches * config.documentsPerBatch
    }`
  );

  if (config.productStructure) {
    const totalWords = config.productStructure.reduce(
      (sum, field) => sum + field.wordCount,
      0
    );
    logger.log(
      `\t- Product structure: ${config.productStructure.length} fields, ${totalWords} total words`
    );

    if (opts.verbose) {
      config.productStructure.forEach((field) => {
        if (field.name === "name") {
          logger.log(`\t\tâ€¢ ${field.name}: <generated by faker>`);
        } else if (field.wordCount > 0) {
          logger.log(`\t\tâ€¢ ${field.name}: ${field.wordCount} words`);
        }
      });
    }
  } else {
    logger.log(
      `\t- Description length: ${
        (config as any).descriptionWordLength || "unknown"
      } lines`
    );
  }

  logger.log(
    `\t- Phases: ${Object.entries(opts.phases)
      .filter(([_, enabled]) => enabled)
      .map(([name]) => name)
      .join(", ")}`
  );

  const client = elasticSdk.createElasticsearchClient();

  try {
    await healthCheck(client, logger);

    logger.log(`ðŸ“š Loading corpus from: ${opts.corpusPath}`);
    const dataGenerator = createDataGenerator(opts.corpusPath);

    if (opts.cleanup.deleteIndexBefore) {
      logger.log(`ðŸ—‘ï¸  Deleting existing index: ${config.indexName}`);
      await elasticSdk.deleteIndex(client, config.indexName);
    }

    logger.log(
      `ðŸ—ï¸  Creating index: ${config.indexName} (type: ${config.indexType})`
    );

    let indexConfig;
    if (config.productStructure) {
      logIndexConfiguration(
        config.indexName,
        config.indexType,
        config.productStructure,
        opts.verbose
      );
      indexConfig = createDynamicIndexConfig(
        config.indexName,
        config.indexType,
        config.productStructure
      );
    } else {
      indexConfig = createIndexConfig(config.indexName, config.indexType);
    }

    await elasticSdk.createIndex(client, indexConfig);

    const context = generateContext({
      config,
      dataGenerator,
      client,
      startTime,
    });

    let indexingResult: IndexingPhaseResult;
    let updateResult: UpdatePhaseResult = {
      totalDocumentsUpdated: 0,
      totalUpdateBatches: 0,
      updateBatchResults: [],
      reindexingTimeMs: 0,
    };
    let searchResult: SearchPhaseResult = {
      fullTextSearchResults: [],
      fuzzySearchResults: [],
    };

    if (opts.phases.indexing) {
      logger.log(`\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•`);
      logger.log(`ðŸ”„ INDEXING PHASE`);
      logger.log(`â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•`);

      indexingResult = await executeIndexingPhase(context);

      context.indexedProducts = indexingResult.indexedProducts;
    } else {
      throw new Error("Indexing phase is required and cannot be skipped");
    }

    if (opts.phases.updates) {
      logger.log(`\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•`);
      logger.log(`ðŸ”„ UPDATE PHASE`);
      logger.log(`â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•`);

      context.dataGenerator.generateProductUpdates = (
        count: number,
        descriptionWordLength?: number
      ) => {
        const availableProducts = context.indexedProducts.slice(0, count);
        return availableProducts.map((product) => ({
          id: product.id,
          updates: config.productStructure
            ? dataGenerator.generateProductUpdate(config.productStructure)
            : dataGenerator.generateProductUpdateLegacy(
                descriptionWordLength || 1
              ),
        }));
      };

      updateResult = await executeUpdatePhase(context);
    }

    if (opts.phases.search) {
      logger.log(`\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•`);
      logger.log(`ðŸ”„ SEARCH PHASE`);
      logger.log(`â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•`);

      searchResult = await executeSearchPhase(context);
    }

    if (opts.cleanup.deleteIndexAfter) {
      logger.log(`\nðŸ—‘ï¸  Cleaning up index: ${config.indexName}`);
      await elasticSdk.deleteIndex(client, config.indexName);
    }

    const totalTestDurationMs = Date.now() - startTime;
    const result: BenchmarkRunnerResult = {
      config,
      indexingData: indexingResult,
      updateData: updateResult,
      searchData: searchResult,
      totalTestDurationMs,
    };

    logger.log(`\nðŸŽ‰ Benchmark completed successfully!`);
    logger.log(
      `â±ï¸  Total duration: ${totalTestDurationMs}ms (${(
        totalTestDurationMs / 1000
      ).toFixed(2)}s)`
    );
    logger.log(`ðŸ“Š Results summary:`);
    logger.log(`\t- Documents indexed: ${indexingResult.totalDocuments}`);
    logger.log(`\t- Documents updated: ${updateResult.totalDocumentsUpdated}`);
    logger.log(
      `\t- Full-text searches: ${searchResult.fullTextSearchResults.length}`
    );
    logger.log(`\t- Fuzzy searches: ${searchResult.fuzzySearchResults.length}`);

    return result;
  } catch (error) {
    try {
      if (opts.cleanup.deleteIndexAfter || opts.cleanup.deleteIndexBefore) {
        await elasticSdk.deleteIndex(client, config.indexName);
      }
    } catch (cleanupError) {
      logger.warn(`âš ï¸  Failed to cleanup index after error:`, cleanupError);
    }

    logger.error(`âŒ Benchmark failed:`, error);
    throw new Error(
      `Benchmark failed: ${
        error instanceof Error ? error.message : String(error)
      }`
    );
  }
};

const healthCheck = async (client: Client, logger: Logger) => {
  logger.log(`ðŸ” Checking Elasticsearch health...`);
  const isHealthy = await elasticSdk.healthCheck(client);
  if (!isHealthy) {
    throw new Error("Elasticsearch is not healthy");
  }
  logger.log(`âœ… Elasticsearch is healthy`);
};

const createIndexConfig = (indexName: string, indexType: IndexType) => {
  switch (indexType) {
    case "standard":
      return createStandardIndexConfig(indexName);
    case "ngram":
      return createNGramIndexConfig(indexName);
    case "stemming":
      return createStemmingIndexConfig(indexName);
    default:
      throw new Error(`Unknown index type: ${indexType}`);
  }
};

const generateContext = ({
  config,
  dataGenerator,
  client,
  startTime,
}: {
  config: BenchmarkConfig;
  dataGenerator: DataGenerator;
  client: Client;
  startTime: number;
}): BenchmarkContext => {
  return {
    client,
    dataGenerator: {
      generateProducts: (count: number, descriptionLength?: number) => {
        if (config.productStructure) {
          return dataGenerator.generateProducts(count, config.productStructure);
        } else {
          return dataGenerator.generateProductsLegacy(
            count,
            descriptionLength || 1
          );
        }
      },
      generateProductsAsync: (count: number, descriptionLength?: number) => {
        if (config.productStructure) {
          return dataGenerator.generateProductsAsync(
            count,
            config.productStructure
          );
        } else {
          return dataGenerator.generateProductsAsyncLegacy(
            count,
            descriptionLength || 1
          );
        }
      },
      generateProductsStream: (count: number, descriptionLength?: number) => {
        if (config.productStructure) {
          return dataGenerator.generateProductsStream(
            count,
            config.productStructure
          );
        } else {
          return dataGenerator.generateProductsStreamLegacy(
            count,
            descriptionLength || 1
          );
        }
      },
      generateProductUpdates: (count: number, descriptionLength?: number) => {
        if (config.productStructure) {
          const updates = dataGenerator.generateProductUpdates(
            count,
            config.productStructure
          );
          return updates.map((update, index) => ({
            id: `temp-id-${index}`,
            updates: update,
          }));
        } else {
          return dataGenerator
            .generateProductUpdatesLegacy(count, descriptionLength || 1)
            .map((update, index) => ({
              id: `temp-id-${index}`,
              updates: update,
            }));
        }
      },
    },
    indexedProducts: [] as Product[],
    startTime,
    config,
  };
};
